{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### About This Tokenizer\n",
    "\n",
    "To tokenize the raw textual review data, we utilize the AutoTokenizer module provided by the transformers package. We initialize the AutoTokenizer with parameters tuned for the pre-trained Google BERT Base Uncased model, as outlined below. We then use the tokenizer to transform the textual review documents into vectors representations of max-length of 256, with shorter sequences padded with a special padding token. To ensure that non-salient tokens (such as the padding, masking, and unknown tokens) do not affect classifier model inference, the tokenizer also produces an attention mask vector for each tokenized document, which are then returned with the tokenized document in a dictionary data structure. "
   ],
   "id": "d3cf0e40eb6a801"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Statements",
   "id": "a0b93773d0a57e37"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-22T11:30:49.085057Z",
     "start_time": "2024-04-22T11:30:47.369849Z"
    }
   },
   "source": [
    "from src.dataset import ReviewDataset\n",
    "from src.dataloader_raw import ReviewDataLoader as ReviewDataLoaderRaw\n",
    "from src.dataloader_tokenized import ReviewDataLoader as ReviewDataLoaderTokenized\n",
    "from src.tokenizer import Tokenizer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e5ce098170b70e50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Environment Variables",
   "id": "ca9ff59f00b6895f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:30:49.088410Z",
     "start_time": "2024-04-22T11:30:49.086068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_path_all = \"../../data/all.csv\"\n",
    "data_path_train = \"../../data/train.csv\"\n",
    "data_path_test = \"../../data/test.csv\"\n",
    "model = \"google-bert/bert-base-uncased\"\n",
    "max_length = 256\n",
    "batch_size = 8"
   ],
   "id": "a0ada1bdf0c51d93",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ae36310f506df6ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Data",
   "id": "4b3cf162f9b2e229"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:30:50.015709Z",
     "start_time": "2024-04-22T11:30:49.088410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = ReviewDataset(data_path_all)\n",
    "dataloader_raw = ReviewDataLoaderRaw(dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "c5c7e5fa8fcd8002",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d40f5dc58ac0d2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Sample",
   "id": "a86d0955a64cdb8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:30:50.099696Z",
     "start_time": "2024-04-22T11:30:50.016003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features, labels = next(iter(dataloader_raw))\n",
    "for i in range(len(features)):\n",
    "    print(f\"Review: {features[i][0:50]}... Sentiment: {labels[i]}\")"
   ],
   "id": "9291662b0c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I&#039;ve tried a few antidepressants over the yea... Sentiment: 1\n",
      "Review: Holy Hell is exactly how I feel. I had been taking... Sentiment: 0\n",
      "Review: This is a waste of money.  Did not curb my appetit... Sentiment: 0\n",
      "Review: No problems, watch what you eat.... Sentiment: 1\n",
      "Review: I smoked for 50+ years.  Took it for one week and ... Sentiment: 1\n",
      "Review: After just 1 dose of this ciprofloxacn, I felt 99%... Sentiment: 1\n",
      "Review: If I could give it a 0, I would absolutely do so. ... Sentiment: 0\n",
      "Review: After a few days and it &quot;kicked in,&quot; eve... Sentiment: 0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a81fa9e49f077eea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Tokenizer",
   "id": "e73d6f845b58b0ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:30:51.537595Z",
     "start_time": "2024-04-22T11:30:50.099696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer(model, max_length)\n",
    "dataloader_tokenized = ReviewDataLoaderTokenized(dataset, tokenizer, batch_size=batch_size, shuffle=False)"
   ],
   "id": "a0b175a5ccb5d18c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6afae7bfba0985c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenized Data Sample",
   "id": "a4430048cd15b54d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:30:51.545219Z",
     "start_time": "2024-04-22T11:30:51.538605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features, labels = next(iter(dataloader_tokenized))\n",
    "tokens = features['input_ids'].tolist()\n",
    "attention_mask = features['attention_mask'].tolist()\n",
    "labels = labels.tolist()\n",
    "for i in range(len(labels)):\n",
    "    print(f\"{{Tokens: {tokens[i][0:5]}... Attention Mask: {attention_mask[i][0:5]}...}} Label: {labels[i]}\")"
   ],
   "id": "b250bd151f99dada",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Tokens: [101, 1045, 1004, 1001, 6021]... Attention Mask: [1, 1, 1, 1, 1]...} Label: 1\n",
      "{Tokens: [101, 4151, 3109, 2003, 3599]... Attention Mask: [1, 1, 1, 1, 1]...} Label: 0\n",
      "{Tokens: [101, 2023, 2003, 1037, 5949]... Attention Mask: [1, 1, 1, 1, 1]...} Label: 0\n",
      "{Tokens: [101, 2053, 3471, 1010, 3422]... Attention Mask: [1, 1, 1, 1, 1]...} Label: 1\n",
      "{Tokens: [101, 1045, 20482, 2005, 2753]... Attention Mask: [1, 1, 1, 1, 1]...} Label: 1\n",
      "{Tokens: [101, 2044, 2074, 1015, 13004]... Attention Mask: [1, 1, 1, 1, 1]...} Label: 1\n",
      "{Tokens: [101, 2065, 1045, 2071, 2507]... Attention Mask: [1, 1, 1, 1, 1]...} Label: 0\n",
      "{Tokens: [101, 2044, 1037, 2261, 2420]... Attention Mask: [1, 1, 1, 1, 1]...} Label: 0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "adeaf92ffd8c1100"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decoded Tokenized Data Sample",
   "id": "54ff2743e11c677"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:30:53.511817Z",
     "start_time": "2024-04-22T11:30:51.545219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(len(labels)):\n",
    "    decoded = tokenizer.get_tokenizer().decode(tokens[i], skip_special_tokens=False)\n",
    "    print(f\"Decoded: {decoded[0:50]} | Label: {labels[i]}\")"
   ],
   "id": "a36c4a8abd3631b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded: [CLS] i & # 039 ; ve tried a few antidepressants o | Label: 1\n",
      "Decoded: [CLS] holy hell is exactly how i feel. i had been  | Label: 0\n",
      "Decoded: [CLS] this is a waste of money. did not curb my ap | Label: 0\n",
      "Decoded: [CLS] no problems, watch what you eat. [SEP] [PAD] | Label: 1\n",
      "Decoded: [CLS] i smoked for 50 + years. took it for one wee | Label: 1\n",
      "Decoded: [CLS] after just 1 dose of this ciprofloxacn, i fe | Label: 1\n",
      "Decoded: [CLS] if i could give it a 0, i would absolutely d | Label: 0\n",
      "Decoded: [CLS] after a few days and it & quot ; kicked in,  | Label: 0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "17ea398d93493184"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenizer Exploration",
   "id": "95ba791dba7bd0b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:30:53.515174Z",
     "start_time": "2024-04-22T11:30:53.511817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Vocab Size: {tokenizer.get_vocab_size()}\")\n",
    "print(f\"Pad Token: {tokenizer.get_pad_token()} | Pad Token ID: {tokenizer.get_pad_token_id()}\")\n",
    "print(f\"Unk Token: {tokenizer.get_unk_token()} | Unk Token ID: {tokenizer.get_unk_token_id()}\")\n",
    "print(f\"Mask Token: {tokenizer.get_mask_token()} | Mask Token ID: {tokenizer.get_mask_token_id()}\")"
   ],
   "id": "6732daad86d50e01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 30522\n",
      "Pad Token: [PAD] | Pad Token ID: 0\n",
      "Unk Token: [UNK] | Unk Token ID: 100\n",
      "Mask Token: [MASK] | Mask Token ID: 103\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7497ecad1179c44e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenized Data Exploration",
   "id": "6787b0e313c0ec0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T11:30:53.526829Z",
     "start_time": "2024-04-22T11:30:53.515174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Number of Reviews: {len(dataset)}\")\n",
    "print(f\"Number of Tokenized Reviews: {len(dataloader_tokenized) * dataloader_tokenized.batch_size}\")\n",
    "\n",
    "features, labels = next(iter(dataloader_tokenized))\n",
    "tokens = features['input_ids'].tolist()\n",
    "attention_mask = features['attention_mask'].tolist()\n",
    "labels = labels.tolist()\n",
    "print(f\"Tokens Length: {len(tokens[0])} | Attention Mask Length: {len(attention_mask[0])}\")\n",
    "    \n",
    "print(\"The number of reviews and tokenized reviews are not the same because the last batch is smaller than the batch size.\")"
   ],
   "id": "9b1ffd3d13ca6fdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Reviews: 96923\n",
      "Number of Tokenized Reviews: 96928\n",
      "Tokens Length: 256 | Attention Mask Length: 256\n",
      "The number of reviews and tokenized reviews are not the same because the last batch is smaller than the batch size.\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
